training_data = [
    ['Sunny', 'Hot', 'High', 'False', 'No'],
    ['Sunny', 'Hot', 'High', 'True', 'No'],
    ['Overcast', 'Hot', 'High', 'False', 'Yes'],
    ['Rainy', 'Mild', 'High', 'False', 'Yes'],
    ['Rainy', 'Cool', 'Normal', 'False', 'Yes'],
    ['Rainy', 'Cool', 'Normal', 'True', 'No'],
    ['Overcast', 'Cool', 'Normal', 'True', 'Yes'],
    ['Sunny', 'Mild', 'High', 'False', 'No'],
    ['Sunny', 'Cool', 'Normal', 'False', 'Yes'],
    ['Rainy', 'Mild', 'Normal', 'False', 'Yes'],
    ['Sunny', 'Mild', 'Normal', 'True', 'Yes'],
    ['Overcast', 'Mild', 'High', 'True', 'Yes'],
    ['Overcast', 'Hot', 'Normal', 'False', 'Yes'],
    ['Rainy', 'Mild', 'High', 'True', 'No']
]

training_data = [
    ['Yes', 'Yes', 10, 'C1'],
    ['Yes', 'Yes', 60, 'C1'],
    ['Yes', 'No', 50, 'C2'],
    ['No', 'No', 40, 'C1'],
    ['No', 'Yes', 70, 'C2'],
    ['No', 'Yes', 30, 'C2'],
    ['No', 'No', 80, 'C2'],
    ['Yes', 'No', 70, 'C1'],
    ['No', 'Yes', 50, 'C2'],
]

x0 = [[-1], [-2]]
B0 = [[1,0],[0,1]]
tolerance = 5 * 10 ** -5

def f(x):
    return 12 * (x[0][0] ** 2) + 4 * (x[1][0] ** 2) - 12 * x[0][0] * x[1][0] + 2 * x[0][0]

def grad_f(x):
    return [[24 * x[0][0] - 12 * x[1][0] + 2],[8 * x[1][0] - 12 * x[0][0]]]

def alphak(x, d):
    num = -2 * d[0][0] * (12 * x[0][0] - 6 * x[1][0] + 1) - (4 * d[1][0] * (2 * x[1][0] - 3 * x[0][0]))
    den = 2 * (12 * (d[0][0] ** 2) + 4 * (d[1][0] ** 2) - 12 * d[0][0] * d[1][0])
    return num / den

backpropagation data 
training_in = [[[1],[0.05],[0.10]], [[1],[0.3],[0.5]]]     # column vectors, already augmented
training_out = [[[0.01],[0.99]],[[0.5],[0.01]]]